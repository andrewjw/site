---
layout: post
title: CouchDB Performance
date: 2008-09-19 12:17:31.000000000 +01:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories: []
tags:
- couchdb
- performance
meta:
  _edit_last: '364050'
author:
  login: andrewjw
  email: andrewjwilkinson@gmail.com
  display_name: Andrew Wilkinson
  first_name: Andrew
  last_name: Wilkinson
permalink: "/2008/09/19/couchdb-performance/"
---
<p>I've been toying with <a href="http://couchdb.org/">CouchDB</a> for a short while, and I'm definitely impressed by what I've seen. Once I'd upgraded to <a href="http://www.erlang.org/">Erlang</a> R12B and trunk CouchDB any bugs I was seeing disappearing and importing all 1 million documents was straightforward.</p>
<p>With 1 million documents the map/reduce takes a long time, as you would expect. What would be nice is if the maps could be spread across different nodes to speed things up dramatically. Once the map has been calculated and cached, retrieving it is relatively fast. Parsing it in Python does seem to be quite slow, taking a few seconds for a few tens of thousands of results. This is far too slow for a webpage response.</p>
<p>Is there any way to speed up CouchDB? Well aggressive use of memcache will probably help, but too me it seems that CouchDB is not suited to large datasets. I do hope I'm wrong though, and I'm going to investigate further because I really want to find a use for CouchDB in my work.</p>
